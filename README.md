# Running Phi3-mini using Olive and ONNX Runtime for efficient inferencing on mobile and web platforms.
This repo is for sharing all the materials for supporting Demo session DEM709 in Microsoft Build 2024.

- Step1 Getting optimized models for mobile and web platforms  
[Optimizing Phi-3 for chosen hardware with Olive​](https://github.com/microsoft/Olive/tree/main/examples/phi3)

- Step2 Running phi-3 on Android phone
- [Runing Phi-3 with ONNX Runtime mobile on Android phone](https://github.com/microsoft/onnxruntime-inference-examples/tree/main/mobile/examples/phi-3/android)

- Step3 Running phi-3 in the browser
- [Runing Phi-3 with ONNX Runtime Web in the browser](https://github.com/microsoft/onnxruntime-inference-examples/tree/gs/chat/js/chat)​

## Learn more
- [ONNX Runtime](https://onnxruntime.ai/) ​

ONNX Runtime | Home​

ONNX Runtime GenAI API​

Generative AI (Preview) | onnxruntime​

Azure Machine learning for Edge​

Deploy AI and machine learning at the edge - Azure Architecture Center | Microsoft Learn ​
