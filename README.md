# Running Phi3-mini using Olive and ONNX Runtime for efficient inferencing on mobile and web platforms.
This repo is to share all the materials for supporting Demo session DEM709 in Microsoft Build 2024.

- Part 1: Getting optimized models for mobile and web platforms
[Optimizing Phi-3 for chosen hardware with Olive​](https://github.com/microsoft/Olive/tree/main/examples/phi3)

- Part 2: Running phi-3 on Android phone
[Android chat app with Phi-3 and ONNX Runtime Mobile](https://github.com/microsoft/onnxruntime-inference-examples/tree/main/mobile/examples/phi-3/android)

- Part 3: Running phi-3 in the browser
[Web chat app with Phi-3 and ONNX Runtime Web](https://github.com/microsoft/onnxruntime-inference-examples/tree/gs/chat/js/chat)

[Blog: Enjoy the Power of Phi-3 with ONNX Runtime on your device](https://huggingface.co/blog/Emma-N/enjoy-the-power-of-phi-3-with-onnx-runtime)​

## Learn more
- [Olive](https://microsoft.github.io/Olive/index.html)
- [ONNX Runtime](https://onnxruntime.ai/) ​
- [ONNX Runtime Generate API](https://github.com/microsoft/onnxruntime-genai)
- [Azure Machine learning for Edge](https://learn.microsoft.com/en-us/azure/architecture/ai-ml/idea/deploy-ai-ml-azure-stack-edge)
​
