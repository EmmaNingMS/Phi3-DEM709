# Running Phi3-mini using Olive and ONNX Runtime for efficient inferencing on mobile and web platforms.
This repo is for sharing all the materials for supporting Demo session DEM709 in Microsoft Build 2024.

- Step1 Getting optimized models for mobile and web platforms  
[Optimizing Phi-3 for chosen hardware with Olive​](https://github.com/microsoft/Olive/tree/main/examples/phi3)

- Step2 Running phi-3 on Android phone
[Android chat APP with Phi-3 and ONNX Runtime Mobile](https://github.com/microsoft/onnxruntime-inference-examples/tree/main/mobile/examples/phi-3/android)

- Step3 Running phi-3 in the browser
[Web chat APP with Phi-3 and ONNX Runtime Web](https://github.com/microsoft/onnxruntime-inference-examples/tree/gs/chat/js/chat)​

## Learn more
- [ONNX Runtime](https://onnxruntime.ai/) ​
- [ONNX Runtime GenAI](https://github.com/microsoft/onnxruntime-genai)
- [Azure Machine learning for Edge](https://learn.microsoft.com/en-us/azure/architecture/ai-ml/idea/deploy-ai-ml-azure-stack-edge)​
